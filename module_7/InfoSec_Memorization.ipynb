{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "-HsHGPWZBFtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # mount your google drive to get permanent storage for your results\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  RESULTS_PATH = \"/content/drive/MyDrive/infoseclab_ML/results\"\n",
        "except ModuleNotFoundError:\n",
        "  RESULTS_PATH = \"results\"\n",
        "\n",
        "!mkdir -p {RESULTS_PATH}"
      ],
      "metadata": {
        "id": "QHH8AofBE9Ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d00b6d5-6b47-48e5-9c12-f373b60b59e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Download the lab files\n",
        "![ ! -d 'infoseclab' ] && git clone https://github.com/ethz-privsec/infoseclab.git\n",
        "%cd infoseclab\n",
        "!git pull https://github.com/ethz-privsec/infoseclab.git\n",
        "%cd ..\n",
        "if \"infoseclab\" not in sys.path:\n",
        "  sys.path.append(\"infoseclab\")"
      ],
      "metadata": {
        "id": "qkfrTYZ7BHBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "235946b7-80d6-4f62-bf8b-71397c33c2d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'infoseclab'...\n",
            "remote: Enumerating objects: 316, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 316 (delta 11), reused 27 (delta 9), pack-reused 281\u001b[K\n",
            "Receiving objects: 100% (316/316), 64.87 MiB | 14.21 MiB/s, done.\n",
            "Resolving deltas: 100% (137/137), done.\n",
            "/content/infoseclab\n",
            "From https://github.com/ethz-privsec/infoseclab\n",
            " * branch            HEAD       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "3UFYO94QBIKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import infoseclab\n",
        "from infoseclab import extraction, Vocab, PREFIX\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "# we won't need gradients here so let's disable them to make things faster\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# utilities for loading & saving results\n",
        "def read_results():\n",
        "  with open(os.path.join(RESULTS_PATH, \"extraction.json\"), \"r\") as f:\n",
        "    res = json.load(f)\n",
        "  return res\n",
        "\n",
        "\n",
        "def write_results(res):\n",
        "  assert len(res) == 4\n",
        "  assert type(res) == dict\n",
        "  with open(os.path.join(RESULTS_PATH, \"extraction.json\"), \"w\") as f:\n",
        "    res = json.dump(res, f)\n",
        "\n",
        "\n",
        "def print_results(res):\n",
        "  for key, value in res.items():\n",
        "    print(f\"{key.replace('_', ' ')}: {repr(value)}\")"
      ],
      "metadata": {
        "id": "qN2qQU8dBMG7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create file to save results"
      ],
      "metadata": {
        "id": "0xYlh_fn7ETQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  res = read_results()\n",
        "  assert len(res) == 4\n",
        "  assert type(res) == dict\n",
        "except FileNotFoundError:\n",
        "  res = {\n",
        "      \"main_character\": None,\n",
        "      \"greedy_guess\": None,\n",
        "      \"greedy_numeric_guess\": None,\n",
        "      \"exact_guess\": None\n",
        "  }\n",
        "  write_results(res)\n",
        "\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "AmNr00RV7D4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e24078bc-6512-4f12-cedc-06590126d24c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: None\n",
            "greedy guess: None\n",
            "greedy numeric guess: None\n",
            "exact guess: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.&nbsp;Freeform generation\n",
        "\n",
        "We will be working with a simple *character-level* language model.\n",
        "\n",
        "This is a model that takes as input a sentence (e.g., \"my name is \") and outputs a distribution over the next character in the sentence. We can then generate a character (e.g., \"F\") by sampling from this distribution. By applying the model recursively to its own output we can generate text character by character: \"my name is Florian\".\n",
        "\n",
        "Technically, the langauge model doesn't operate on `characters` but on `tokens` (numbers). The characters in the model's \"vocabulary\" are sorted, and can thus be referenced by an integer. The i-th value in the langauge model's output corresponds to the probability assigned to the i-th character in the vocabulary.\n",
        "\n",
        "You can find the full vocabulary (i.e., all characters that the language model can produce) in `infoseclab.extraction.Vocab`.\n",
        "This class has two utility dictionaries, `char_to_ix` and `ix_to_char` for converting from a character to its index (its token) and vice-versa:\n",
        "\n",
        "```\n",
        "Vocab.char_to_ix['a'] -> 54\n",
        "Vocab.ix_to_char[54] -> 'a'\n",
        "```"
      ],
      "metadata": {
        "id": "Y1PXWtQ-BnGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load a simple character-level language model\n",
        "lm = extraction.load_lm(\"infoseclab/data/secret_model.pth\", device=device)"
      ],
      "metadata": {
        "id": "3z_zuKMQ37V1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of how to generate text from the language model\n",
        "extraction.generate(lm, \"my name is\", length =500)\n",
        "\n",
        "# London, Baker Street, Watson, Sherlock => Sherlock Holmes"
      ],
      "metadata": {
        "id": "qT7c-NI57MSX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "901bef3c-9894-4c5f-df76-6550acde1517"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my name is\\n to visit eager his sombre is Garia, I can carry you are in me to lile he\\n came back in our key and browing with a vague demember that a plain was mercy.\\n The rails to be conscience of lost, forese it saw the north of\\n America, and yet not call him in the matter. What is German of the deal fiery of a\\n fog is 3244,\" he meanly drifted at the passed his hand sobbled, standily. \"But about\\n Company has proin is Utvernidiestic walking sound of\\n possesses and what is enough to make through the Mr. Sta'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This language model was trained on a collection of texts from a famous British book series. \n",
        "Your first goal is to figure out which books.**\n",
        "\n",
        "**Your guess should be in the form `\"Firstname Lastname\"` of the book series' main character.\n",
        "For example, if you guessed that the book series is Harry Potter, then your guess would be `\"Harry Potter\"`.**\n",
        "\n",
        "Note: the code immediately below doesn't check for correctness! It just checks that you've made a guess."
      ],
      "metadata": {
        "id": "zKL8VWwn4gVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "guess = \"Sherlock Holmes\"\n",
        "res = read_results()\n",
        "res['main_character'] = guess\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "cXRYQ3B6Bpti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea3df8b-9a32-4dcc-876a-80af8f5a341a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: None\n",
            "greedy numeric guess: None\n",
            "exact guess: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.&nbsp;Secret extraction\n",
        "\n",
        "Unfortunately, the training data from this language model also contained the sentence `\"Florian's password is XXXXX\"`. (the real password is blanked out, your goal is to recover it!)\n",
        "\n",
        "The model might have *memorized* the correct password, and your goal will be to recover it.\n",
        "\n",
        "For this, you know the *prefix*: `\"Florian's password is \"`\n",
        "(you can find this stored under `infoseclab.extraction.PREFIX`).\n",
        "\n",
        "You also know that Florian's password is exactly 5 characters long (so that it it easier to memorize, *obviously*)."
      ],
      "metadata": {
        "id": "K3c1YfON5bx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1&nbsp; Greedy secret extraction\n",
        "\n",
        "You will first attempt to extract the secret password *greedily*, simply by sampling the **5 most likely characters**, one-by-one, from the language model, starting from the known `PREFIX`.\n",
        "\n",
        "You can use the `extraction.generate` method as inspiration for this.\n",
        "\n",
        "*Note that `extraction.generate` does <b>not</b> sample greedily from the model. Rather, it samples a character at random according to the probability distribution predicted by the model.*"
      ],
      "metadata": {
        "id": "XLR_DCUHDt5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_greedy(lm, prompt, length=5):\n",
        "    \"\"\"\n",
        "    Generate a sequence of characters by sampling from the language model.\n",
        "    :param lm: the language model\n",
        "    :param prompt: the prompt to start the generation from\n",
        "    :param length: the number of characters to generate\n",
        "    :return: the prompt concatenated with the generated sequence\n",
        "    \"\"\"\n",
        "    generated_text = \"\"\n",
        "    hidden_state = None\n",
        "\n",
        "    # tokenize the prompt\n",
        "    input_seq = [Vocab.char_to_ix[ch] for ch in prompt]\n",
        "    # tensor of dimension (N,) where N is the number of characters in the prompt\n",
        "    input_seq = torch.tensor(input_seq).to(lm.device)\n",
        "\n",
        "    for i in range(length):\n",
        "        # forward pass through the model\n",
        "        # output is a tensor of dimension (N, vocab_size)\n",
        "        output, hidden_state = lm.forward(input_seq, hidden_state)\n",
        "        # get a distribution over the next character\n",
        "        # probas is of dimension (vocab_size,)\n",
        "        probas = F.softmax(output[-1], dim=0)\n",
        "\n",
        "        # most likely\n",
        "        index = torch.argmax(probas)\n",
        "        generated_text += Vocab.ix_to_char[index.item()]\n",
        "\n",
        "        # to continue the generation, we simply evaluate\n",
        "        # the model on the last predicted character,\n",
        "        # and the current state\n",
        "        input_seq = torch.tensor([index.item()]).to(lm.device)\n",
        "        \n",
        "    return generated_text\n",
        "\n",
        "guess_greedy = generate_greedy(lm, PREFIX, length=5)\n",
        "print(\"greedy:\", PREFIX + repr(guess_greedy))\n",
        "\n",
        "res = read_results()\n",
        "res['greedy_guess'] = guess_greedy\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "2tq-2nO8D0Z9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4089dd8c-7397-4df2-fcac-ea4a6f780002"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "greedy: Florian's password is '3\\n an'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: None\n",
            "exact guess: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2&nbsp;Greedy numeric secret extraction\n",
        "\n",
        "Your greedy extraction likely generated some giberish! (but hey, a password might genuinely look like that).\n",
        "\n",
        "You are now given some extra information: **Florian's password only contains numbers!** (he's not very good at security).\n",
        "\n",
        "Modify your greedy sampling mechanism to repeatedly sample the 5 most likely *numbers*, one-by-one, starting from the known `PREFIX`."
      ],
      "metadata": {
        "id": "wCtm2C2L5jep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_greedy_numeric(lm, prompt, length=5):\n",
        "    \"\"\"\n",
        "    Generate a sequence of characters by sampling from the language model.\n",
        "    :param lm: the language model\n",
        "    :param prompt: the prompt to start the generation from\n",
        "    :param length: the number of characters to generate\n",
        "    :return: the prompt concatenated with the generated sequence\n",
        "    \"\"\"\n",
        "    generated_text = \"\"\n",
        "    hidden_state = None\n",
        "\n",
        "    # tokenize the prompt\n",
        "    input_seq = [Vocab.char_to_ix[ch] for ch in prompt]\n",
        "    # tensor of dimension (N,) where N is the number of characters in the prompt\n",
        "    input_seq = torch.tensor(input_seq).to(lm.device)\n",
        "\n",
        "    for i in range(length):\n",
        "        # forward pass through the model\n",
        "        # output is a tensor of dimension (N, vocab_size)\n",
        "        output, hidden_state = lm.forward(input_seq, hidden_state)\n",
        "\n",
        "        # get a distribution over the next character\n",
        "        # probas is of dimension (vocab_size,)\n",
        "        probas = F.softmax(output[-1], dim=0)\n",
        "\n",
        "        for i in range(12, 22):\n",
        "          probas[i] +=1\n",
        "\n",
        "        index = torch.argmax(probas)\n",
        "        generated_text += Vocab.ix_to_char[index.item()]\n",
        "\n",
        "        # to continue the generation, we simply evaluate\n",
        "        # the model on the last predicted character,\n",
        "        # and the current state\n",
        "        input_seq = torch.tensor([index.item()]).to(lm.device)\n",
        "    return generated_text\n",
        "\n",
        "guess_greedy_numeric = generate_greedy_numeric(lm, PREFIX, length=5)\n",
        "print(\"greedy (numeric):\", PREFIX + repr(guess_greedy_numeric))\n",
        "\n",
        "res = read_results()\n",
        "res['greedy_numeric_guess'] = guess_greedy_numeric\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "6Pbzx4dSa1sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ede017a7-bab3-473a-8751-024d3b71a5ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "greedy (numeric): Florian's password is '39731'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.3&nbsp;Exact numeric secret extraction\n",
        "\n",
        "Spoiler alert: the secret you found using greedy sampling is *not* Florian's password.\n",
        "\n",
        "As it turns out, sampling greedily from the model is not guaranteed to find the *sequence* of characters that is most likely according to the model's probability distribution.\n",
        "\n",
        "To illustrate, below you can compare the loss from your greedy guess, and a different (also incorrect) guess.</br>\n",
        "The guess `\"36175\"` has lower loss!"
      ],
      "metadata": {
        "id": "16tSQO1RHBxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(guess_greedy_numeric, extraction.get_loss(lm, PREFIX + guess_greedy_numeric))\n",
        "print(\"36175\", extraction.get_loss(lm, PREFIX + \"36175\"))\n"
      ],
      "metadata": {
        "id": "J5xuvMF7HFg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30dda4ae-a711-433c-c0b4-087249cc4cce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39731 tensor(0.9791, device='cuda:0')\n",
            "36175 tensor(0.8980, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the final part, find the 5-digit secret that actually *minimizes* the model's loss, when prompted with the `PREFIX`."
      ],
      "metadata": {
        "id": "IkmUuKQWbaVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def generate_exact(lm, prompt, length=5):\n",
        "    \"\"\"\n",
        "    Generate a sequence of characters by sampling from the language model.\n",
        "    :param lm: the language model\n",
        "    :param prompt: the prompt to start the generation from\n",
        "    :param length: the number of characters to generate\n",
        "    :return: the prompt concatenated with the generated sequence\n",
        "    \"\"\"\n",
        "    generated_text = \"\"\n",
        " \n",
        "    mini_loss = 10e5\n",
        "    \n",
        "    for k in tqdm(range(100000)):\n",
        "       k = str(k).zfill(length)\n",
        "       loss = extraction.get_loss(lm, PREFIX + k)\n",
        "       if loss < mini_loss:\n",
        "         generated_text = k\n",
        "         mini_loss = loss\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "guess_exact = generate_exact(lm, PREFIX, length=5)\n",
        "print(\"\\nexact:\", PREFIX + repr(guess_exact))\n",
        "\n",
        "res = read_results()\n",
        "res['exact_guess'] = guess_exact\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "CjLjFgyTIzgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95adc6a0-24ab-4278-d8be-08eda4e0044f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100000/100000 [03:23<00:00, 490.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "exact: Florian's password is '35192'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create submission file (**upload `results.zip` to moodle**) \n"
      ],
      "metadata": {
        "id": "fNMIfOoL_dOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -j -r \"{RESULTS_PATH}/results.zip\" {RESULTS_PATH} --exclude \"*x_adv_untargeted.npy\""
      ],
      "metadata": {
        "id": "S0N1Uv1Y_cLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c965fd1f-bdb9-4d2d-c2d5-f42bf83be86b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: x_adv_targeted.npy (deflated 10%)\n",
            "updating: x_adv_detect.npy (deflated 10%)\n",
            "updating: x_adv_jpeg.npy (deflated 10%)\n",
            "updating: x_adv_random.npy (deflated 12%)\n",
            "  adding: extraction.json (deflated 25%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile(f\"{RESULTS_PATH}/results.zip\", 'r') as zip:\n",
        "    res = json.load(zip.open(\"extraction.json\"))\n",
        "    print_results(res)"
      ],
      "metadata": {
        "id": "VSPUajuP_zcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ceb2030-7f39-4058-e4be-63765b679b95"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    }
  ]
}